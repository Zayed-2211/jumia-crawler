{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import lxml\n",
    "from os.path import exists\n",
    "import os\n",
    "from pandas import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Files Creating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_products_file():\n",
    "    file_name = \"Products.csv\"\n",
    "    with open(file_name, \"w\", newline='', encoding=\"utf-8\") as file:\n",
    "        wr = csv.writer(file)\n",
    "        wr.writerow([\"Product\", \"Price\", \"General category\", \"Sub category\", \"Category\", \"Brand\", \"Specifications\", \"Rating\", \"Reviews\", \"Link\", \"Seller\", \"Seller Score\", \"Seller link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_done_file():\n",
    "    file_name = \"Done.csv\"\n",
    "    with open(file_name, \"w\", newline='', encoding=\"utf-8\") as file:\n",
    "        wr = csv.writer(file)\n",
    "        wr.writerow(['Sub category'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sellers_file():\n",
    "    file_name = \"Sellers.csv\"\n",
    "    with open(file_name, \"w\", newline='', encoding=\"utf-8\") as file:\n",
    "        wr = csv.writer(file)\n",
    "        wr.writerow(['Seller', 'Score', 'Profile link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check files existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_files_exist():\n",
    "    products_file_exists = os.path.exists('Products.csv')\n",
    "    if not products_file_exists:\n",
    "        create_products_file()\n",
    "    \n",
    "    done_file_exists = os.path.exists('Done.csv')\n",
    "    if not done_file_exists:\n",
    "        create_done_file()\n",
    "    \n",
    "    seller_file_exists = os.path.exists('Sellers.csv')\n",
    "    if not done_file_exists:\n",
    "        create_sellers_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_products_file(category_name):\n",
    "    \n",
    "    with open('Products.csv', 'a', newline='', encoding=\"utf-8\") as file:\n",
    "        wr = csv.writer(file)\n",
    "        \n",
    "        with open(category_name, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            category_data = list(reader)\n",
    "            \n",
    "            for i in range(0, len(category_data)):\n",
    "                wr.writerow(category_data[i])\n",
    "                \n",
    "            f.close()\n",
    "        file.close()\n",
    "        \n",
    "        os.remove(category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_category_file(sub_category):\n",
    "    file_name = sub_category + \".csv\"\n",
    "    with open(file_name, \"w\", newline='', encoding=\"utf-8\") as file:\n",
    "        wr = csv.writer(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_done_file(sub_category):\n",
    "    with open(\"Done.csv\", 'a', newline='', encoding=\"utf-8\") as file:\n",
    "        wr = csv.writer(file)\n",
    "        wr.writerow([sub_category])\n",
    "        file.close()\n",
    "    print(\"------\", sub_category.replace(\".csv\", \"\"), \"crawling is done! -----\")\n",
    "        \n",
    "    append_products_file(sub_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sellers_file(seller_info):\n",
    "    with open(\"Sellers.csv\", 'a', newline='', encoding=\"utf-8\") as file:\n",
    "        wr = csv.writer(file)\n",
    "        wr.writerow(seller_info)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sub_category_file(file_name, new_row):\n",
    "    file_name = file_name + \".csv\"\n",
    "    with open(file_name, 'a', newline='', encoding=\"utf-8\") as file:\n",
    "        wr = csv.writer(file)\n",
    "        wr.writerow(new_row)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_this_page(url):\n",
    "    result = requests.get(url)\n",
    "\n",
    "    src = result.content\n",
    "    soup = BeautifulSoup(src, \"lxml\")\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_exists(urlLink):\n",
    "    \n",
    "    defSoup = get_this_page(urlLink)\n",
    "    defSoup = defSoup.text\n",
    "    \n",
    "    return(\"No results found!\" not in defSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pages_exceeded(firstLinkPart, idx = 50):  \n",
    "\n",
    "    print(\"Checking:\", firstLinkPart)\n",
    "    \n",
    "    fullUrlLink = firstLinkPart + \"?page=\" + str(idx) + \"#catalog-listing\"\n",
    "\n",
    "    print(\"Max exceeded:\", page_exists(fullUrlLink))\n",
    "    return(page_exists(fullUrlLink))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page(current_url, current_index):\n",
    "\n",
    "    next_index = current_index + 1\n",
    "    \n",
    "    current_index = '=' + str(current_index) + '#'\n",
    "    next_index = '=' + str(next_index) + '#'\n",
    "    \n",
    "    next_url = current_url.replace(current_index, next_index)\n",
    "    if page_exists(next_url):\n",
    "        return(next_url)\n",
    "    else:\n",
    "        return(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Crawling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_product_page(url_link, crawled_sellers):\n",
    "    seller_name = seller_score = seller_link = product_specifications = \"NaN\"\n",
    "    \n",
    "    soup = get_this_page(url_link)\n",
    "    \n",
    "    cont = soup.find(\"main\", {\"class\": \"-pvs\"})\n",
    "        \n",
    "    if cont == None:\n",
    "        return seller_name, seller_score, seller_link, product_specifications\n",
    "        \n",
    "    specs = cont.find(\"div\", {\"class\": \"row -pas\"})\n",
    "    if specs != None:\n",
    "        product_specifications = specs.get_text()\n",
    "    \n",
    "    if \"col4\" not in str(cont):\n",
    "        return seller_name, seller_score, seller_link, product_specifications\n",
    "    \n",
    "    sellerBox = cont.find(\"div\", {\"class\": \"col4\"})\n",
    "    \n",
    "    name = sellerBox.find(\"p\", {\"class\": \"-m -pbs\"})\n",
    "    seller_name = name.get_text()\n",
    "    \n",
    "    # check_seller(seller_name)\n",
    "    \n",
    "    if seller_name == 'Jumia':\n",
    "        seller_score = '100%'\n",
    "        return seller_name, seller_score, seller_link, product_specifications\n",
    "    \n",
    "    score = sellerBox.find(\"bdo\", {\"class\": \"-m -prxs\"})\n",
    "    if score != None:\n",
    "        seller_score = score.get_text()\n",
    "        \n",
    "    link = sellerBox.find(\"a\", {\"class\": \"-pas -df -i-ctr -upp\"})\n",
    "    if link != None:\n",
    "        seller_link = \"https://www.jumia.com.eg\" + str(link.get('href'))\n",
    "    \n",
    "    if seller_name not in crawled_sellers:\n",
    "        crawled_sellers.append(seller_name)\n",
    "        write_to_sellers_file([seller_name, seller_score, seller_link])\n",
    "    \n",
    "    return seller_name, seller_score, seller_link, product_specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_page(category_name, url_link, general_category_name, crawled_sellers):\n",
    "\n",
    "    print(\"Now crawling:\", url_link)\n",
    "    \n",
    "    soup = get_this_page(url_link)\n",
    "    cont = soup.find(\"div\", {\"class\": \"-paxs row _no-g _4cl-3cm-shs\"})\n",
    "\n",
    "    if \"a class=\\\"core\\\"\" not in str(cont):\n",
    "        return\n",
    "\n",
    "    products = cont.find_all(\"a\", {\"class\": \"core\"})\n",
    "\n",
    "    for product in products:\n",
    "        name = product.find(\"h3\", {\"class\": \"name\"}).get_text()\n",
    "\n",
    "        if name:\n",
    "            price = product.find(\"div\", {\"class\": \"prc\"}).get_text()\n",
    "            link = \"https://www.jumia.com.eg\" + str(product.get(\"href\"))\n",
    "            \n",
    "            seller_name, seller_score, seller_link, specifications = crawl_product_page(link, crawled_sellers)\n",
    "            \n",
    "            if \"Brand:\" in str(product):\n",
    "                brand = str(product.get(\"data-brand\"))\n",
    "            else:\n",
    "                brand = \"NaN\"\n",
    "                \n",
    "            if \"stars _s\" in str(product):\n",
    "                rating = product.find(\"div\", {\"class\": \"stars _s\"}).get_text()\n",
    "                reviews = product.find(\"div\", {\"class\": \"rev\"}).get_text()\n",
    "                rating = rating.replace(\" out of 5\", \"\")\n",
    "                reviews = reviews.replace(rating, \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\" out of \", \"\")\n",
    "            else:\n",
    "                rating = \"NaN\"\n",
    "                reviews = \"NaN\"\n",
    "\n",
    "            full_category_names = str(product.get(\"data-category\"))\n",
    "            full_category_names = full_category_names.split('/')\n",
    "            \n",
    "            if len(full_category_names) >= 2:\n",
    "                sub_category_name = full_category_names[-2]\n",
    "                final_category = full_category_names[-1]\n",
    "            else:\n",
    "                sub_category_name = category_name\n",
    "                final_category = category_name\n",
    "                \n",
    "            new_product_row = [name, price, general_category_name, sub_category_name, final_category, brand, specifications, rating, reviews, link,  seller_name, seller_score, seller_link]\n",
    "            write_to_sub_category_file(category_name, new_product_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_category(category_name, url_link, general_category_name, crawled_categories, crawled_sellers):\n",
    "\n",
    "    if (category_name in crawled_categories):\n",
    "        print(\"-----------------------------\", category_name, \"is already crawled! -----------------------------\")\n",
    "        return\n",
    "    \n",
    "    create_sub_category_file(category_name)\n",
    "    \n",
    "    url_link = url_link + \"?page=1#catalog-listing\"\n",
    "    \n",
    "    current_index = 1\n",
    "    \n",
    "    while url_link:\n",
    "        \n",
    "        crawl_page(category_name, url_link, general_category_name, crawled_sellers)\n",
    "        \n",
    "        url_link = get_next_page(url_link, current_index)\n",
    "\n",
    "        current_index += 1        \n",
    "    \n",
    "    file_name = category_name + \".csv\"\n",
    "    write_to_done_file(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Break category function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_category(crawled_categories, big_category, general_category_name, crawled_sellers):\n",
    "\n",
    "    soup = get_this_page(big_category)\n",
    "\n",
    "    sub_div = soup.find(\"div\", {\"class\": \"col4 -me-start -pvs\"})\n",
    "\n",
    "    if \"-db -pvs -phxl -hov-bg-gy05\" in str(sub_div):\n",
    "        temp_sub_categories = sub_div.find_all(\"a\", {\"class\": \"-db -pvs -phxl -hov-bg-gy05\"})\n",
    "    elif \"-db -pvs -phm -hov-bg-gy05\" in str(sub_div):\n",
    "        temp_sub_categories = sub_div.find_all(\"a\", {\"class\": \"-db -pvs -phm -hov-bg-gy05\"})\n",
    "    else:\n",
    "        temp_sub_categories = []\n",
    "                \n",
    "    for temp_sub_category in temp_sub_categories:\n",
    "        sub_category_link = \"https://www.jumia.com.eg\" + str(temp_sub_category.get(\"href\"))\n",
    "        category_name = temp_sub_category.get(\"href\").replace(\"/seller/\", \"\").replace(\"/\", \"\")\n",
    "        print(sub_category_link)\n",
    "\n",
    "        if category_name + '.csv' in crawled_categories:\n",
    "            print(\"-----------------------------\", category_name, \"is already crawled! -----------------------------\")\n",
    "            \n",
    "        elif max_pages_exceeded(sub_category_link) or (category_name == \"I/O Port Cards\"):\n",
    "            print(\"------- Breaking category:\", category_name)\n",
    "            break_category(crawled_categories, sub_category_link, general_category_name, crawled_sellers)\n",
    "            \n",
    "        else:\n",
    "            category_name = temp_sub_category.get(\"href\").replace(\"/seller/\", \"\").replace(\"/\", \"\")\n",
    "            print(\"--------------- Category:\", category_name)\n",
    "            crawl_category(category_name, sub_category_link, general_category_name, crawled_categories, crawled_sellers)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Run program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.jumia.com.eg/seller/\"\n",
    "soup = get_this_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_files_exist()\n",
    "\n",
    "categories_data = read_csv(\"Done.csv\")\n",
    "crawled_categories = categories_data['Sub category'].tolist()\n",
    "\n",
    "sellers_data = read_csv(\"Sellers.csv\")\n",
    "crawled_sellers = sellers_data['Seller'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_categories_links = []\n",
    "general_categories_names = []\n",
    "\n",
    "cont = soup.find(\"div\", {\"class\": \"card -fh\"})\n",
    "general_categories = cont.find_all(\"a\", {\"class\": \"-db -pvs -phxl -hov-bg-gy05\"})\n",
    "\n",
    "for category in general_categories:\n",
    "    category_link = str(category.get(\"href\"))\n",
    "    \n",
    "    if category_link != \"None\":\n",
    "        general_categories_links.append(\"https://www.jumia.com.eg\" + category_link)\n",
    "        general_categories_names.append(category.get_text())\n",
    "\n",
    "categories_count = len(general_categories_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if '-db -pvs -phxl -hov-bg-gy05' in str(soup):\n",
    "      print('yes')\n",
    "else:\n",
    "  print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(categories_count):\n",
    "    if general_categories_names[i] != 'Crawled!':\n",
    "        break_category(crawled_categories, general_categories_links[i], general_categories_names[i], crawled_sellers)\n",
    "        general_categories_names[i] = 'Crawled!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92db6153dbf7ec7b87e4d014dbbd3b36977afe20bdf5d52bb46bf1cabc22a040"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
